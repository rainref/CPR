{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fd1fc366",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████| 106052/106052 [00:00<00:00, 625550.16it/s]\n",
      "100%|███████████████████████████████| 106237/106237 [00:00<00:00, 502427.94it/s]\n",
      "  2%|▋                                   | 2104/106051 [00:03<02:29, 693.03it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 315\u001b[0m\n\u001b[1;32m    312\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m new_results\n\u001b[1;32m    314\u001b[0m format_lines\u001b[38;5;241m=\u001b[39mconvert_format(allline_gen,allline_ans)\n\u001b[0;32m--> 315\u001b[0m parser_lines\u001b[38;5;241m=\u001b[39m\u001b[43mgraph_parser\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformat_lines\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    316\u001b[0m converted_lines\u001b[38;5;241m=\u001b[39mdata_convertor(parser_lines)\n\u001b[1;32m    317\u001b[0m final_result\u001b[38;5;241m=\u001b[39mmake(converted_lines)\n",
      "Cell \u001b[0;32mIn[55], line 174\u001b[0m, in \u001b[0;36mgraph_parser\u001b[0;34m(allline)\u001b[0m\n\u001b[1;32m    172\u001b[0m begin_pos\u001b[38;5;241m=\u001b[39msp0\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<extra_id_0>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    173\u001b[0m end_pos\u001b[38;5;241m=\u001b[39mbegin_pos\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mlen\u001b[39m(sp_ref\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m--> 174\u001b[0m res_text,range_marker,token_list,answer_text\u001b[38;5;241m=\u001b[39m\u001b[43mparse_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbegin_pos\u001b[49m\u001b[43m,\u001b[49m\u001b[43mend_pos\u001b[49m\u001b[43m,\u001b[49m\u001b[43msp_res\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    175\u001b[0m result \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnodes\u001b[39m\u001b[38;5;124m\"\u001b[39m: res_text,\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mline_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: index,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    182\u001b[0m \n\u001b[1;32m    183\u001b[0m }\n\u001b[1;32m    185\u001b[0m res_line\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mstr\u001b[39m(result))\n",
      "Cell \u001b[0;32mIn[55], line 129\u001b[0m, in \u001b[0;36mparse_text\u001b[0;34m(text, begin_pos, end_pos, sp_res)\u001b[0m\n\u001b[1;32m    127\u001b[0m now_idx,id_dict,range_marker,attr_dict\u001b[38;5;241m=\u001b[39mtrav_tree_make_number(tree\u001b[38;5;241m.\u001b[39mroot_node,begin_pos\u001b[38;5;241m=\u001b[39mbegin_pos,end_pos\u001b[38;5;241m=\u001b[39mend_pos,id_dict\u001b[38;5;241m=\u001b[39m{},range_marker\u001b[38;5;241m=\u001b[39m[])\n\u001b[1;32m    128\u001b[0m string_dict,result_dict\u001b[38;5;241m=\u001b[39m{},{}\n\u001b[0;32m--> 129\u001b[0m res\u001b[38;5;241m=\u001b[39m\u001b[43mgenerate_node_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot_node\u001b[49m\u001b[43m,\u001b[49m\u001b[43mid_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43mresult_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresult_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43mstring_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstring_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43mattr_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattr_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m answer_list\u001b[38;5;241m=\u001b[39m[]\n\u001b[1;32m    131\u001b[0m tree_answer \u001b[38;5;241m=\u001b[39m parser\u001b[38;5;241m.\u001b[39mparse(\u001b[38;5;28mbytes\u001b[39m(sp_res\u001b[38;5;241m.\u001b[39mencode()))\n",
      "Cell \u001b[0;32mIn[55], line 105\u001b[0m, in \u001b[0;36mgenerate_node_text\u001b[0;34m(root_node, id_dict, result_dict, string_dict, sub_mapping, attr_dict)\u001b[0m\n\u001b[1;32m    103\u001b[0m maybe_children \u001b[38;5;241m=\u001b[39m[]\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m child \u001b[38;5;129;01min\u001b[39;00m root_node\u001b[38;5;241m.\u001b[39mchildren:\n\u001b[0;32m--> 105\u001b[0m     \u001b[43mgenerate_node_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchild\u001b[49m\u001b[43m,\u001b[49m\u001b[43mid_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43mresult_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43mstring_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43msub_mapping\u001b[49m\u001b[43m,\u001b[49m\u001b[43mattr_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(child) \u001b[38;5;129;01min\u001b[39;00m id_dict:\n\u001b[1;32m    107\u001b[0m         maybe_children\u001b[38;5;241m.\u001b[39mappend(id_dict[\u001b[38;5;28mstr\u001b[39m(child)])\n",
      "Cell \u001b[0;32mIn[55], line 105\u001b[0m, in \u001b[0;36mgenerate_node_text\u001b[0;34m(root_node, id_dict, result_dict, string_dict, sub_mapping, attr_dict)\u001b[0m\n\u001b[1;32m    103\u001b[0m maybe_children \u001b[38;5;241m=\u001b[39m[]\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m child \u001b[38;5;129;01min\u001b[39;00m root_node\u001b[38;5;241m.\u001b[39mchildren:\n\u001b[0;32m--> 105\u001b[0m     \u001b[43mgenerate_node_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchild\u001b[49m\u001b[43m,\u001b[49m\u001b[43mid_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43mresult_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43mstring_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43msub_mapping\u001b[49m\u001b[43m,\u001b[49m\u001b[43mattr_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(child) \u001b[38;5;129;01min\u001b[39;00m id_dict:\n\u001b[1;32m    107\u001b[0m         maybe_children\u001b[38;5;241m.\u001b[39mappend(id_dict[\u001b[38;5;28mstr\u001b[39m(child)])\n",
      "    \u001b[0;31m[... skipping similar frames: generate_node_text at line 105 (2 times)]\u001b[0m\n",
      "Cell \u001b[0;32mIn[55], line 105\u001b[0m, in \u001b[0;36mgenerate_node_text\u001b[0;34m(root_node, id_dict, result_dict, string_dict, sub_mapping, attr_dict)\u001b[0m\n\u001b[1;32m    103\u001b[0m maybe_children \u001b[38;5;241m=\u001b[39m[]\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m child \u001b[38;5;129;01min\u001b[39;00m root_node\u001b[38;5;241m.\u001b[39mchildren:\n\u001b[0;32m--> 105\u001b[0m     \u001b[43mgenerate_node_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchild\u001b[49m\u001b[43m,\u001b[49m\u001b[43mid_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43mresult_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43mstring_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43msub_mapping\u001b[49m\u001b[43m,\u001b[49m\u001b[43mattr_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(child) \u001b[38;5;129;01min\u001b[39;00m id_dict:\n\u001b[1;32m    107\u001b[0m         maybe_children\u001b[38;5;241m.\u001b[39mappend(id_dict[\u001b[38;5;28mstr\u001b[39m(child)])\n",
      "Cell \u001b[0;32mIn[55], line 84\u001b[0m, in \u001b[0;36mgenerate_node_text\u001b[0;34m(root_node, id_dict, result_dict, string_dict, sub_mapping, attr_dict)\u001b[0m\n\u001b[1;32m     81\u001b[0m             now_idx,_,_,_\u001b[38;5;241m=\u001b[39mtrav_tree_make_number(child,now_idx\u001b[38;5;241m=\u001b[39mnow_idx,id_dict\u001b[38;5;241m=\u001b[39mid_dict,begin_pos\u001b[38;5;241m=\u001b[39mbegin_pos,end_pos\u001b[38;5;241m=\u001b[39mend_pos,range_marker\u001b[38;5;241m=\u001b[39mrange_marker,attr_dict\u001b[38;5;241m=\u001b[39mattr_dict)\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m now_idx,id_dict,range_marker,attr_dict\n\u001b[0;32m---> 84\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_node_text\u001b[39m(root_node,id_dict,result_dict\u001b[38;5;241m=\u001b[39m{},string_dict\u001b[38;5;241m=\u001b[39m{},sub_mapping\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,attr_dict\u001b[38;5;241m=\u001b[39m{}):\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(root_node) \u001b[38;5;129;01min\u001b[39;00m attr_dict \u001b[38;5;129;01mand\u001b[39;00m attr_dict[\u001b[38;5;28mstr\u001b[39m(root_node)]:\n\u001b[1;32m     86\u001b[0m         prefix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf!:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import json\n",
    "import tree_sitter\n",
    "from tree_sitter import Language, Parser\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "JAVA_LANGUAGE = Language('/root/chatgpt/tree-sitter/tree-sitter/build/my-languages.so', 'java')\n",
    "\n",
    "parser = Parser()\n",
    "parser.set_language(JAVA_LANGUAGE)\n",
    "import pickle\n",
    "\n",
    "\n",
    "id_dict={}\n",
    "max_num=0\n",
    "\n",
    "ref_file=\"/root/S2RCC/data/test_java_construct.tsv\"\n",
    "answer_file=\"/root/chatgpt/test_java_construct_gpt_regen_result_3.tsv\"\n",
    "out_file=\"/root/LLM/full_data/test_java_token_oldmode_tokenlist.pkl\"\n",
    "\n",
    "# convert format\n",
    "with open(answer_file,mode=\"r\",encoding=\"utf-8\") as f:\n",
    "    allline_gen=f.readlines()\n",
    "with open(ref_file,mode=\"r\",encoding=\"utf-8\") as f:\n",
    "    allline_ans=f.readlines()\n",
    "\n",
    "def convert_format(allline_gen,allline_ans):\n",
    "    gen_dict={}\n",
    "    for line in tqdm(allline_gen):\n",
    "        sps=line.split(\"\\t\")\n",
    "        if len(sps)!=2:\n",
    "            continue\n",
    "        src,gen_data=sps\n",
    "        gen_dict[src]=gen_data\n",
    "    for line in tqdm(allline_ans):\n",
    "        src,answer=line.split(\"\\t\")\n",
    "        if src in gen_dict:\n",
    "            gen_data = gen_dict[src]\n",
    "            if isinstance(gen_data,tuple):\n",
    "                continue\n",
    "            gen_dict[src]=(answer,gen_data)\n",
    "\n",
    "    format_lines=[]\n",
    "    for key,value in gen_dict.items():\n",
    "        if not isinstance(value,tuple):\n",
    "            continue\n",
    "        text = json.dumps(key) + \"\\t\" + json.dumps(value[0].replace(\"\\n\",\"\"))+ \"\\t\" + json.dumps(value[1].replace(\"\\n\",\"\"))\n",
    "        format_lines.append(text)\n",
    "    return format_lines\n",
    "\n",
    "def trav_tree_make_text(root_node,res_list=None,string_dict=None):\n",
    "    global max_num\n",
    "    if len(root_node.children)==0:\n",
    "        res_list.append((root_node.text.decode(),root_node.start_point[1]))\n",
    "    elif root_node.type == 'string_literal':\n",
    "        node_text=root_node.text.decode()\n",
    "        if node_text not in string_dict:\n",
    "            string_dict[node_text] = \"STRING{}\".format(len(string_dict))\n",
    "        res_list.append((string_dict[node_text] ,root_node.start_point[1]))\n",
    "    else:\n",
    "        for child in root_node.children:\n",
    "            trav_tree_make_text(child,res_list,string_dict)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def trav_tree_make_number(root_node,begin_pos=-1,end_pos=-1,now_idx=-1,id_dict={},range_marker=[],attr_dict={}):\n",
    "    attr_dict[str(root_node)] = False\n",
    "    if root_node.start_point[1] >= begin_pos and root_node.end_point[1] <= end_pos:\n",
    "        attr_dict[str(root_node)] = True\n",
    "    if not root_node.is_named:\n",
    "        assert len(root_node.children)==0\n",
    "        return now_idx,id_dict,range_marker,attr_dict\n",
    "    if str(root_node) in id_dict:\n",
    "        return now_idx, id_dict, range_marker,attr_dict\n",
    "    now_idx+=1\n",
    "    id_dict[str(root_node)] = now_idx\n",
    "\n",
    "    if root_node.type != 'string_literal':\n",
    "        for child in root_node.children:\n",
    "            now_idx,_,_,_=trav_tree_make_number(child,now_idx=now_idx,id_dict=id_dict,begin_pos=begin_pos,end_pos=end_pos,range_marker=range_marker,attr_dict=attr_dict)\n",
    "    return now_idx,id_dict,range_marker,attr_dict\n",
    "\n",
    "def generate_node_text(root_node,id_dict,result_dict={},string_dict={},sub_mapping=None,attr_dict={}):\n",
    "    if str(root_node) in attr_dict and attr_dict[str(root_node)]:\n",
    "        prefix=\"f!:\"\n",
    "    else:\n",
    "        prefix=\"\"\n",
    "    res_dict={}\n",
    "    result_dict[str(root_node)]=res_dict\n",
    "    if sub_mapping is None:\n",
    "        sub_mapping={}\n",
    "    if len(root_node.children)>0:\n",
    "        if root_node.type == 'string_literal':\n",
    "            text = root_node.text.decode()\n",
    "            if text not in string_dict:\n",
    "                string_dict[text] = \"STRING{}\".format(len(string_dict))\n",
    "            text = prefix+string_dict[text]\n",
    "            res_dict['value']=[(text,root_node.start_point[1])]\n",
    "            for child in root_node.children:\n",
    "                sub_mapping[str(child)]=str(root_node)\n",
    "            return result_dict\n",
    "        maybe_children =[]\n",
    "        for child in root_node.children:\n",
    "            generate_node_text(child,id_dict,result_dict,string_dict,sub_mapping,attr_dict)\n",
    "            if str(child) in id_dict:\n",
    "                maybe_children.append(id_dict[str(child)])\n",
    "        if len(maybe_children)>0:\n",
    "            res_dict['children']=maybe_children\n",
    "    else:\n",
    "        if not root_node.is_named:\n",
    "            parent_text=str(root_node.parent)\n",
    "            if parent_text not in result_dict:\n",
    "                parent_text=sub_mapping[parent_text]\n",
    "            parent_dict=result_dict[parent_text]\n",
    "            if \"value\" not in parent_dict:\n",
    "                parent_dict['value']=[]\n",
    "            parent_dict['value'].append((prefix+root_node.text.decode(),root_node.start_point[1]))\n",
    "            return result_dict\n",
    "        else:\n",
    "            res_dict['value']=[(prefix+root_node.text.decode(),root_node.start_point[1])]\n",
    "    res_dict['type']=prefix+root_node.type\n",
    "    return  result_dict\n",
    "\n",
    "def parse_text(text,begin_pos=-1,end_pos=-1,sp_res=None):\n",
    "    tree = parser.parse(bytes(text.encode()))\n",
    "    now_idx,id_dict,range_marker,attr_dict=trav_tree_make_number(tree.root_node,begin_pos=begin_pos,end_pos=end_pos,id_dict={},range_marker=[])\n",
    "    string_dict,result_dict={},{}\n",
    "    res=generate_node_text(tree.root_node,id_dict,result_dict=result_dict,string_dict=string_dict,attr_dict=attr_dict)\n",
    "    answer_list=[]\n",
    "    tree_answer = parser.parse(bytes(sp_res.encode()))\n",
    "    trav_tree_make_text(tree_answer.root_node,answer_list,string_dict)\n",
    "    answer_list.sort(key=lambda x: x[1])\n",
    "    answer_text=\" \".join([i[0] for i in answer_list])\n",
    "\n",
    "    id_tuple=list(id_dict.items())\n",
    "    id_tuple.sort(key=lambda x:x[1])\n",
    "    id_dict_order=[i[0] for i in id_tuple]\n",
    "    res_by_order=[res[i] for i in id_dict_order]\n",
    "    token_list=[]\n",
    "    for item in res_by_order:\n",
    "        if \"value\" in item:\n",
    "            token_list.extend(item['value'])\n",
    "            # item['value']=[i[0] for i in item['value']]\n",
    "    token_list.sort(key=lambda x : x[1])\n",
    "    begin_idx=0\n",
    "    end_idx=-1\n",
    "    for idx in range(len(token_list)-1):\n",
    "        if token_list[idx][1] <= begin_pos and token_list[idx+1][1] > begin_pos:\n",
    "            begin_idx=idx\n",
    "        if token_list[idx-1][1] < end_pos and token_list[idx][1] >= end_pos:\n",
    "            end_idx = idx\n",
    "    token_list.insert(end_idx,(\"</ref>\",end_pos))\n",
    "    token_list.insert(begin_idx,(\"<ref>\", begin_pos))\n",
    "    \n",
    "\n",
    "    return res_by_order,range_marker,token_list,answer_text\n",
    "\n",
    "\n",
    "def graph_parser(allline):\n",
    "    const_len=len(\"public class test { \")\n",
    "    error_counter=0\n",
    "    res_line=[]\n",
    "    for index,line in enumerate(tqdm(allline)):\n",
    "        sp = line.split(\"\t\")\n",
    "        if len(sp) < 2: continue\n",
    "        elif len(sp)==3:\n",
    "            sp_src,sp_res,sp_ref=sp\n",
    "            sp_src,sp_res,sp_ref=json.loads(sp_src),json.loads(sp_res),json.loads(sp_ref)\n",
    "            sp0 =  \"public class test { \" + sp_src + \"}\"\n",
    "            new_text = sp0.replace(\"<extra_id_0>\", sp_ref.replace(\"\\n\", \"\"))\n",
    "            begin_pos=sp0.find(\"<extra_id_0>\")\n",
    "            end_pos=begin_pos+len(sp_ref.replace(\"\\n\", \"\"))\n",
    "            res_text,range_marker,token_list,answer_text=parse_text(new_text,begin_pos,end_pos,sp_res)\n",
    "            result = {\n",
    "                \"nodes\": res_text,\n",
    "                \"line_id\": index,\n",
    "                \"mark_range\": range_marker,\n",
    "                \"text\":\" \".join([i[0].replace(\"f!:\",\"\") for i in token_list]),\n",
    "                # 'text': sp0.replace(\"<extra_id_0>\", \" <ref> \" + sp_ref.replace(\"\\n\", \"\") + \" </ref> \"),\n",
    "                \"answer\": answer_text\n",
    "\n",
    "            }\n",
    "\n",
    "            res_line.append(str(result))\n",
    "    return res_line\n",
    "\n",
    "def data_convertor(lines):\n",
    "    reses_line=[]\n",
    "    for line in tqdm(lines):\n",
    "        json_line = eval(line)\n",
    "        if isinstance(json_line, dict):\n",
    "            old_json_line = json_line\n",
    "            code_index = json_line['line_id']\n",
    "            mark_range = json_line['mark_range']\n",
    "            text_code = json_line['text']\n",
    "            answer_data = json_line['answer']\n",
    "            json_line = json_line[\"nodes\"]\n",
    "        else:\n",
    "            code_index = None\n",
    "        new_data = {\"nodes\": [], \"edges\": {\n",
    "            \"Child\": [],\n",
    "            \"InField\": [],\n",
    "            \"NextToken\": [],\n",
    "            \"NextUse\": []\n",
    "        },\n",
    "                    # \"token_seq\":[],\n",
    "                    # \"type_seq\":[],\n",
    "                    \"token_type_map\": []}\n",
    "        token_node = []\n",
    "        type_node = []\n",
    "        last_token = -1\n",
    "        old_new_mapping = {}\n",
    "        last_use_mapping = {}\n",
    "        idx = 0\n",
    "        if isinstance(json_line, str):\n",
    "            json_line = json.loads(json_line)\n",
    "        for origin_idx, nd in enumerate(json_line):\n",
    "            if nd == 0:\n",
    "                break\n",
    "            if \"type\" not in nd:\n",
    "                type_str = \"string_literal\"\n",
    "            else:\n",
    "                type_str = nd['type']\n",
    "            type_idx = idx\n",
    "            old_new_mapping[origin_idx] = type_idx\n",
    "            new_data[\"nodes\"].append(type_str)\n",
    "            # new_data[\"type_seq\"].append(type_idx)\n",
    "            idx = idx + 1\n",
    "\n",
    "            if \"children\" in nd.keys():\n",
    "                childrens = nd[\"children\"]\n",
    "                for c in childrens:\n",
    "                    new_data[\"edges\"][\"Child\"].append([type_idx, -c])\n",
    "\n",
    "            if \"value\" in nd.keys():\n",
    "                values = nd[\"value\"]\n",
    "                if not isinstance(values, list):\n",
    "                    values = [values]\n",
    "                for v in values:\n",
    "                    token_idx = idx\n",
    "                    new_data[\"nodes\"].append(v)\n",
    "                    idx = idx + 1\n",
    "                    # new_data[\"token_seq\"].append(token_idx)\n",
    "                    new_data[\"edges\"][\"InField\"].append([type_idx, token_idx])\n",
    "                    if last_token != -1:\n",
    "                        new_data[\"edges\"][\"NextToken\"].append([last_token, token_idx])\n",
    "                    last_token = token_idx\n",
    "                    if v in last_use_mapping.keys():\n",
    "                        new_data[\"edges\"][\"NextUse\"].append([last_use_mapping[v], token_idx])\n",
    "                        last_use_mapping[v] = token_idx\n",
    "                    else:\n",
    "                        last_use_mapping[v] = token_idx\n",
    "                    new_data[\"token_type_map\"].append(type_idx)\n",
    "\n",
    "        for e in new_data[\"edges\"][\"Child\"]:\n",
    "            e[1] = old_new_mapping[-e[1]]\n",
    "\n",
    "        if code_index is not None:\n",
    "            new_data['index'] = code_index\n",
    "        new_data['mark_range'] = mark_range\n",
    "        new_data['text'] = text_code\n",
    "        new_data['answer'] = answer_data\n",
    "\n",
    "        reses_line.append(json.dumps(new_data))\n",
    "    return reses_line\n",
    "\n",
    "def make(lines):\n",
    "    new_results=[]\n",
    "    for line in tqdm(lines):\n",
    "        obj_line = eval(line)\n",
    "        nodes_tuples = [(node, idx) for idx, node in enumerate(obj_line[\"nodes\"])]\n",
    "        tokens_idx = [i[1] for i in obj_line['edges']['InField']]\n",
    "        tokens_idx.sort()\n",
    "        token_nodes=[]\n",
    "        type_nodes=[]\n",
    "        pointer=0\n",
    "        for i in range(len(nodes_tuples)):\n",
    "            if pointer < len(tokens_idx) and i == tokens_idx[pointer]:\n",
    "                token_nodes.append(nodes_tuples[i])\n",
    "                pointer+=1\n",
    "            else:\n",
    "                type_nodes.append(nodes_tuples[i])\n",
    "        token_nodes.sort(key=lambda x:x[0][1])\n",
    "        idx_mapping={}\n",
    "        counter=0\n",
    "        for node in type_nodes+token_nodes:\n",
    "            idx_mapping[node[1]]=counter\n",
    "            counter+=1\n",
    "        new_res={}\n",
    "        new_res['tokens'] = [i[0][0] for i in token_nodes]\n",
    "        new_res['types'] = [i[0] for i in type_nodes]\n",
    "        new_res['mark_range'] = []\n",
    "        new_res['edges']={}\n",
    "        for key in obj_line['edges']:\n",
    "            new_res['edges'][key]=[]\n",
    "            for l in obj_line['edges'][key]:\n",
    "                new_list=(idx_mapping[l[0]],idx_mapping[l[1]])\n",
    "                new_res['edges'][key].append(new_list)\n",
    "        new_res['text']=(obj_line['text'],obj_line['answer'])\n",
    "        for idx_k in range(0,len(new_res['types'])+len(new_res['tokens'])):\n",
    "            if idx_k < len(new_res['types']):\n",
    "                if new_res[\"types\"][idx_k][:3]==\"f!:\":\n",
    "                    new_res[\"types\"][idx_k]=new_res[\"types\"][idx_k][3:]\n",
    "                    new_res['mark_range'].append(idx_k)\n",
    "            else:\n",
    "                if new_res['tokens'][idx_k-len(new_res['types'])][:3]==\"f!:\":\n",
    "                    new_res['tokens'][idx_k-len(new_res['types'])]=new_res['tokens'][idx_k-len(new_res['types'])][3:]\n",
    "                    new_res['mark_range'].append(idx_k)\n",
    "\n",
    "        new_results.append(new_res)\n",
    "    return new_results\n",
    "\n",
    "format_lines=convert_format(allline_gen,allline_ans)\n",
    "parser_lines=graph_parser(format_lines)\n",
    "converted_lines=data_convertor(parser_lines)\n",
    "final_result=make(converted_lines)\n",
    "pickle.dump(final_result, open(out_file, mode=\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ff9a128",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c: \" + compressionCodecClass + \" (not found)\", e); } if (!(CompressionCodec.class.isAssignableFrom(tempClass))) throw new RuntimeException(\"Invalid class for compression codec: \" + compressionCodecClass + \" (not a CompressionCodec)\"); return (Class<? extends CompressionCodec>) tempClass; }\\tClassNotFoundException e\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allline_ans[257][268:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59e80c61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'public static Class<? extends CompressionCodec> getCompressionCodeClass(String compressionCodecClass) { Class<?> tempClass; try { tempClass = Class.forName(compressionCodecClass); } catch ( <extra_id_0>) { throw new RuntimeException(\"Invalid class for compression codec: \" + compressionCodecClass + \" (not found)\", e); } if (!(CompressionCodec.class.isAssignableFrom(tempClass))) throw new RuntimeException(\"Invalid class for compression codec: \" + compressionCodecClass + \" (not a CompressionCodec)\"); return (Class<? extends CompressionCodec>) tempClass; }\\tClassNotFoundException e\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allline_ans[257]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1902d0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "text='''public class test { public static Class<? extends CompressionCodec> getCompressionCodeClass(String compressionCodecClass) { Class<?> tempClass; try { tempClass = Class.forName(compressionCodecClass); } catch ( ClassNotFoundException) { throw new RuntimeException(\"Invalid class for compression codec: \" + compressionCodecClass + \" (not found)\", e); } if (!(CompressionCodec.class.isAssignableFrom(tempClass))) throw new RuntimeException(\"Invalid class for compression codec: \" + compressionCodecClass + \" (not a CompressionCodec)\"); return (Class<? extends CompressionCodec>) tempClass; }}'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "98c398fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "') { throw new RuntimeException(\"Invalid class for compression codec: \" + compressionCodecClass + \" (not found)\", e); } if (!(CompressionCodec.class.isAssignableFrom(tempClass))) throw new RuntimeException(\"Invalid class for compression codec: \" + compressionCodecClass + \" (not a CompressionCodec)\"); return (Class<? extends CompressionCodec>) tempClass; }}'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[232:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0233321b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp0='''public class test { public static Class<? extends CompressionCodec> getCompressionCodeClass(String compressionCodecClass) { Class<?> tempClass; try { tempClass = Class.forName(compressionCodecClass); } catch ( <extra_id_0>) { throw new RuntimeException(\"Invalid class for compression codec: \" + compressionCodecClass + \" (not found)\", e); } if (!(CompressionCodec.class.isAssignableFrom(tempClass))) throw new RuntimeException(\"Invalid class for compression codec: \" + compressionCodecClass + \" (not a CompressionCodec)\"); return (Class<? extends CompressionCodec>) tempClass; }}'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c114157b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'new RuntimeException(\"Invalid class for compression codec: \" + compressionCodecClass + \" (not found)\", e); } if (!(CompressionCodec.class.isAssignableFrom(tempClass))) throw new RuntimeException(\"Invalid class for compression codec: \" + compressionCodecClass + \" (not a CompressionCodec)\"); return (Class<? extends CompressionCodec>) tempClass; }}'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp0[232:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d784601a",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_list=[('public', 0), ('class', 7), ('test', 13), ('{', 18), ('public', 20), ('static', 27), ('Class', 34), ('<', 39), ('?', 40), ('extends', 42), ('CompressionCodec', 50), ('>', 66), ('getCompressionCodeClass', 68), ('(', 91), ('String', 92), ('compressionCodecClass', 99), (')', 120), ('{', 122), ('Class', 124), ('<', 129), ('?', 130), ('>', 131), ('tempClass', 133), (';', 142), ('try', 144), ('{', 148), ('tempClass', 150), ('=', 160), ('Class', 162), ('.', 167), ('forName', 168), ('(', 175), ('compressionCodecClass', 176), (')', 197), (';', 198), ('}', 200), ('catch', 202), ('(', 208), ('f!:ClassNotFoundException', 210), (')', 232), ('f!:', 232), ('{', 234), ('throw', 236), ('new', 242), ('RuntimeException', 246), ('(', 262), ('STRING0', 263), ('+', 303), ('compressionCodecClass', 305), ('+', 327), ('STRING1', 329), (',', 343), ('e', 345), (')', 346), (';', 347), ('}', 349), ('if', 351), ('(', 354), ('!', 355), ('(', 356), ('CompressionCodec', 357), ('.', 373), ('class', 374), ('.', 379), ('isAssignableFrom', 380), ('(', 396), ('tempClass', 397), (')', 406), (')', 407), (')', 408), ('throw', 410), ('new', 416), ('RuntimeException', 420), ('(', 436), ('STRING0', 437), ('+', 477), ('compressionCodecClass', 479), ('+', 501), ('STRING2', 503), (')', 530), (';', 531), ('return', 533), ('(', 540), ('Class', 541), ('<', 546), ('?', 547), ('extends', 549), ('CompressionCodec', 557), ('>', 573), (')', 574), ('tempClass', 576), (';', 585), ('}', 587), ('}', 588)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0f6519a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('f!:ClassNotFoundException', 210)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_list[38]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4f93061f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[37, 38, 39, 40, 41, 42, 82, 83, 84, 85, 86, 87, 88]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_result[20]['mark_range']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4871c78f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('public class test { @ Override public JdbcSession createSession ( ) { JdbcSession session = new JdbcSession ( ) ; if ( this . defaultMaxInactiveInterval != null ) { session . setMaxInactiveInterval ( Duration . ofSeconds ( <ref> this . defaultMaxInactiveInterval . toSeconds ( ) </ref> ) ) ; } return session ; } }',\n",
       " 'this . defaultMaxInactiveInterval ')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_result[20]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f45a2c6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "')'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_result[20]['tokens'][88-len(final_result[20]['types'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64419ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9785cfb9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:python38] *",
   "language": "python",
   "name": "conda-env-python38-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
